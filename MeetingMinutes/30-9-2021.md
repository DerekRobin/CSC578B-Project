## Meeting 1 - September 30, 2021

Tentative weekly meeting time & location going forward: Tues @ 1:30pm - 2:30pm in Clearihue courtyard near the classroom.

Motivation:
- We agree with authors' motivation (developers should know what attributes are associated with long lasting projects), and wish to verify the credibility of the study performed.
- We would like to compare the difference between the analysis performed in the study and a Bayesian approach we have learned about in class to see if they produce similar results.
- If we have time, maybe we can find other attributes that correlate with successful projects, or try new threshold values.

Assigned tasks:
- Manish: Look into resources we can use to perform our experiments (e.g. what server can we host it on, what specs does it need to handle the dataset properly). 
- Derek: Write the proposal.
- Neha: Look for related studies (addressing point 1 of the proposal list in project.md of the course repo) that discuss Kaplan-Meiers estimator and Cox Proportional-Hazards model (may be within the medical field) as well as the topic of software project longevity and health.
- Keanu: Invesigate tools we will need (how to set up postgreSQL and how to work with the dataset in another programming language).

Group tasks:
- Review the related studies Neha finds so that we have a better understanding of the work that has already been done on this.
- Read the proposal to make sure we are all in agreeance about what our project entails and our responsibilities, and to generally proof read it.
- If possible on our personal machines, download and play around with the dataset to understand what is in it and how it is formatted. If not, then wait until we have a server, and play with it there.
- We will all participate in the presentation to practice our presenting skills.

General tasks:
- Book room for presentation (maybe)
- Manish is willing to edit the presentation video

Workflow:
- The report will be written in LaTex.
- We will each be assigned some section(s), and we will do a first pass. After the first draft of the report is done, we will all review the entire document and provide comments for others (or ourselves if we notice anything).
- We plan to split into two groups: Manish and Keanu will work on the replication of the original study, while Neha and Derek will work on the Bayesian approach to the study (determined via coin flip).

Keanu sent Neil this email before the meeting:

```
Hi Neil,

I believe Derek already emailed you regarding our group’s project idea (the replication of http://www1.chapman.edu/~linstead/aliMSR2020.pdf). It sounds like you suggested to get familiar with the methods used in the paper by recreating the study, then if there is time, use some of the analysis techniques we have learned in the class on the dataset to answer the same questions and see how they compare.

I am curious, if someone sets out to do a replication of a study, and everything goes well, what do they do? They can’t write a meaningful paper because what they did didn’t add anything, they can only affirm the findings of the original offer. Does this mean you don’t ever see pure replications of studies (i.e. some twist is always put on the original study)? How would someone affirm the findings of a paper to give it more credibility (I would hope being published in a journal would already lend credibility to a study, but it sounds like the peer reviewing process isn’t the most air tight)?

Sorry, that was a bit of a tangent. What I am trying to ask is, should we plan to do the extra analysis on the data unless our replication finds a major issue with the original study? Also, should we use all the same studied attributes and thresholds as in the paper (e.g. more than or less than 20 developers) or should we try to look at different attributes and change threshold values around? I’m not sure if the attributes map well from the original studies’ methods and the Bayesian methods we plan to apply, so the previous question might not make any sense.

Thank you for listening to my rambling,

Keanu Enns
```


Keanu sent Neil this email after the meeting:
```
Hi again,

Sorry to double email you, but our team just had a meeting, and we have a few questions:
1)	The project.md write up indicates that the presentation is worth 10%, but later indicates it is worth 20%. I believe I saw it listed as 10% in another place (and in a more detailed breakdown), but we just want to be sure we have the correct value.
2)	We have not seen a deadline for the project proposal, though the outline indicates it is due in the 6th week, and I believe I heard you mention October 13th at one point. Would you be able to clarify what the deadline is?
3)	The project is supposed to be 10 pages. Is that 10 pages including everything (e.g. references or title page) or 10 pages of content? Is that supposed to be a lower bound, upper bound, exact amount, or approximate goal? I know personally, I prefer when papers don’t try to pad the content with extraneous information.
4)	Where can we get more information about resources available to us through UVic or other places? We are looking to process a dataset that is roughly 5 GB, so not huge, but a decent enough size to bog down our personal machines when running algorithms on it.

Thank you for all your help, we appreciate it!

Keanu
```

Awaiting responses.