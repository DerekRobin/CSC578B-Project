\documentclass[acmconf]{acmart}
% % Packages
%\usepackage{cite}
\usepackage{listings}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
%\usepackage{parskip}
%\setlength{\parskip}{1em}
\usepackage[bottom]{footmisc}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{totpages}
\usepackage{subfigure}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{10.1145/1122445.1122456}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%   June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

% Specify path to images
\graphicspath{ {./img/} }

% import custom commands found in commands.tex
\input{commands}
% set up commands to format RQ nicely
\newlist{questions}{enumerate}{2}
\setlist[questions,1]{label=\textbf{RQ\arabic*.},ref=RQ\arabic*}
\setlist[questions,2]{label=(\alph*),ref=\thequestionsi(\alph*)}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Survival Analysis of Open Source Projects]{Two Differing Approaches to Survival Analysis of Open Source Python Projects}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author{Derek Robinson}
\email{drobinson@uvic.ca}
\author{Keanelek Enns}
\email{keanelekenns@uvic.ca}
\author{Neha Koulecar}
\email{nehakoulecar@uvic.ca}
\author{Manish Sihag}
\email{manishsihag@uvic.ca}
\affiliation{%
  \institution{\\University of Victoria}
  \department{Computer Science}
  \streetaddress{PO Box 1700 STN CSC}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8W 2Y2}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{D. Robinson, K. Enns, N. Koulecar, M. Sihag}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\keanu{Abstract Pending}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011134.10003559</concept_id>
<concept_desc>Software and its engineering~Open source model</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003227.10003351</concept_id>
<concept_desc>Information systems~Data mining</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Open source model}
\ccsdesc[300]{Information systems~Data mining}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{data science, survival analysis, open source, python, Kaplan Meier, Cox proportional hazards model, Bayesian analysis}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction} \label{intro}

The developers of Open Source Software (OSS) projects are often part of decentralized and geographically distributed teams of volunteers.
As these developers volunteer their free time to build such OSS projects, they likely want to be confident that the projects they work on will not become inactive.
Suppose OSS developers are aware of key attributes that are associated with long-lasting projects. In that case, they can make informed assessments of a given project before devoting their time to it, or they can strive to make their own projects exhibit those attributes.
Understanding which attributes of an OSS project lead to its longevity motivated Ali \emph{et al.} to apply survival analysis techniques commonly found in biostatistics to study the probability of survival for popular OSS Python projects \cite{ali2020cheating}.
Ali \emph{et al.} (referred to as the original authors from here on) specifically studied the effect of the following attributes on the survival of OSS Python projects: publishing major releases, the use of multiple hosting services, the type of hosting service, and the size of the volunteer team.

Survival analysis is a set of methods used to determine how long an entity will live (or the time to a given event of interest, such as death) and is often used in the medical field.
For example, survival analysis can determine the probability of a patient surviving past a certain time when given a treatment.
However, death is not as well defined for a software project as it is for a living organism.
A project may not receive revisions for an extended period only to be returned to at a later date, or perhaps a project no longer receives revisions at all, but the community that uses it continues to be active.
Samoladas \emph{et al.} considered a project inactive if it received less than two revisions a month; two months of inactivity led to it being considered abandoned or dead \cite{samoladas2010survival}.
Evangelopoulos \emph{et al.} and the original authors considered a project dead once there were no revisions at all \cite{evangelopoulos, ali2020cheating}.
The latter definition of project abandonment or death is used in this study to measure the duration of a project or its survival.

The original authors use a frequentist approach to survival analysis utilizing such methods as the Kaplan-Meier (K-M) survival estimator and the Cox Proportional-Hazards model \cite{kaplan1958nonparametric, cox1972regression}.
Though frequentist approaches are considered to be unbiased, minimal in variance, efficient, and generally sufficient, some consider them to lack robustness \cite{renganathan2016overview}. Another approach to survival analysis, Bayesian analysis, is considered to generate more robust models that perform well under the presence of new data being introduced and are generally easier to interpret results from \cite{renganathan2016overview}.
\keanu{We will flesh this discussion out a bit more and use more citations}
\derek{idk if we need more citations, I think we just need to give a bit of backgroud into KM curves and Cox model}

The authors of this paper resonate with the motivation of the original authors. 
This paper serves as a replication of their paper \cite{ali2020cheating} (referred to as the original paper from here on) and seeks to assess the validity of their analysis.
This replication also provides artifacts so that others may see how the study was conducted and reproduce it with ease.
For the sake of the authors' curiosity, an additional attribute of the data was analyzed: the revision frequency of the project.
In addition to the replication, this paper analyzes the same data set using a Bayesian approach to survival analysis as outlined in \cite{kelter2020bayesian} and seeks to compare the results of the frequentist and Bayesian approaches in the same domain.
Thus, the research questions this paper answers are as follows:

\begin{questions}
    \item How do major releases, the use of multiple hosting services, the type of hosting service, and the size of the volunteer team affect the probability of survival of an OSS Python project?
    \item How does the revision frequency of an OSS Python project affect the probability of its survival?
    \item How do the findings of frequentist survival analysis differ from Bayesian survival analysis?
\end{questions}

Section \ref{related} outlines other research which has utilized survival analysis to study OSS and other work which has studied attributes similar to those studied in this paper.
Section \ref{data} describes the source of the data set, the data set itself, and the required preparation in order to perform survival analysis.
Section \ref{methods} covers the methods used for the replication, additional frequentist analysis, and Bayesian analysis.
Section \ref{results} shows the results for each analysis.
Section \ref{discussion} discusses the comparative differences between the analyses performed, the limitations of the study, and suggestions for future work on the topic.
The final section concludes by summarizing the purpose and findings of this paper.

\section{Related Work} \label{related}

Several other researchers have employed survival analysis to study the health of OSS projects. 
For example, Samoladas \emph{et al.} studied the affect of application domain and developer count on OSS project health \cite{samoladas2010survival}. 
They found that applications within the domain of \emph{games and entertainment} and \emph{security} had the lowest probability of survival. 
Additionally, they found that for each new developer introduced to a project, the projects survivability increased by 15.8\%. 
On the topic of developers, several studies have used survival analysis to study developer disengagement from OSS projects \cite{miller2019people,lin2017developer,ortega2009survival}. 
Miller \emph{et al.} made use of a survey and survival analyses, to determine the causes behind why a developer might stop contributing to an OSS project \cite{miller2019people}. 
Their analysis revealed that developers have a higher probability of disengagement when going through job transitions and working longer hours. 
Lin \emph{et al.} determined that developers who balance maintaining files they have created with maintaining files other have created have a higher survival probability than developers who only maintain their own files or only maintain others files \cite{lin2017developer}. Additionally, Lin \emph{et al.} found that developers who maintained files and developers who mainly wrote code have a higher survival probability than those who solely created files and those who main focus is writing documentation. 
Ortega and Izquierdo-Cortazar analyzed the survival of FLOSS committers and Wikipedia editors and found that FLOSS committers have higher mean survival times than Wikipedia editors \cite{ortega2009survival}. 
Survival analysis can also be applied to the software it self, this has been demonstrated by Aman \emph{et al.} and Caivano \emph{et al.} \cite{aman2017survival, caivano2021exploratory}. 
In \cite{aman2017survival}, survival analysis was used to analyze time to bug-fix for files modified by developers of different experience levels. 
This analysis determined that files which most recently modified by less experienced developers had an increased probability of needing a bug fix within a shorter time frame \cite{aman2017survival}. 
Caivano \emph{et al.} explored the affect of dead code within OSS project using survival analysis \cite{caivano2021exploratory}. 
They found that dead methods are present in Java code, persist for a long time (in terms of commits) before being buried or revived, are rarely revived, and that most dead methods have been dead since their inception. 

Several studies have examined the health of OSS projects using methods other than survival analysis. \derek{This transition sectence needs some work b\c i feel like 2 does not equal several.}
Xia \emph{et al.} predicted a number of health attributes of OSS projects, such as, the number of developers and the number of revisions. 
These predictions were made using regression trees that were optimized using differential evolution, leading to a 10\% increase in accuracy over the base line \cite{xia2020predicting}. 
Norick \emph{et al.} analyzed OSS projects using code quality measures and observed no significant evidence that the number of committing developers affects software quality \cite{norick2010effects}. 

\section{Data} \label{data}

Performing survival analysis of OSS projects requires a data set that records the repositories for projects on common hosting sites, including a history of all commits (revisions from here on out) and major releases (revisions of note, often with a specific name and release date) \cite{ali2020cheating}. 
The \emph{popular-3k-python} subset of the Software Heritage graph \cite{pietri2019software} contained the necessary information and was used in the original paper and also in this paper.
This data set contains information on 3052 popular Python projects hosted on GitHub/GitLab, Debian, and PyPI between 2005 and 2018.
Following the tutorial provided by the Software Heritage organization \cite{SQLdataset}, a PostgreSQL database was hosted on the first author's local machine to facilitate data collection.

Though the \emph{popular-3k-python} data set contains all the necessary information to perform survival analysis, it first must be manipulated into a more suitable format before the analysis can be carried out. 
In this case, the collected data was manipulated such that our final data set contained the duration of the project, the censorship value, and the attributes of interest. 
Descriptions of each column present in the dataset can be found in Table \ref{tab:data}. 
Data collection and manipulation were performed in Jupyter Notebooks, which are available in this paper's repository, a link to which can be found in appendix \ref{artifacts}. 

\begin{table}[h!]
    \caption{Data Set Column Descriptions}
    \label{tab:data}
    \begin{tabular}{ll}
        \toprule
        Column Name & Description \\
        \midrule
        Host Type            & Which hosting service the projects repository resides on \\
        Major Releases       & Whether or not the project publishes major releases \\
        Censored             & Whether or not the project death is observed (for more information see section \ref{replmethods}) \\
        Duration\_months     & The duration of the project in months \\
        High\_rev\_frequency & Whether or not the project has high revision frequency (greater than one revision a day) \\
        Multi\_repo          & Whether or not the project is hosted on multiple \\
        High\_author\_count  & Whether or not the project has a high author count (greater than twenty unique authors) \\
        \bottomrule
    \end{tabular}
\end{table}

%For example, most of the analyzed features or attributes of the projects in the original paper are not present in the raw data set and need to be calculated. 
%The work done in this study has been documented in a repository for the benefit of the reader.
%References to the repository can be found in appendix \ref{artifacts}, and the jupyter notebook used to perform the data manipulation can be found in Analysis\&Data/data-collection.ipynb.

%Queries were created in order to join the necessary tables and extract the relevant values.
%The queries accomplish the following objectives: they group records by the \emph{url} field of the origin table (used to identify projects), filter records to be within the desired time frame, count distinct authors that have participated in a given project, record the host service (depicted by the \emph{type} field of the origin table), record the earliest and latest revisions associated with each project url, count the number of revisions made during each project's duration, and identify whether major releases were published.

%The notebook uses popular python libraries such as psycopg2, pandas, numpy, and matplotlib to connect to the database, extract the results of the previously mentioned queries, manipulate and filter the data, calculate new fields, and plot results.

\section{Methods} \label{methods}

\subsection{Replication} \label{replmethods}

Two critical concepts in survival analysis are events of interest and censoring.
In the case of this paper, as well as the original paper, the event of interest is the abandonment of a project.
That is, if a project stops receiving revisions, it is considered abandoned.
However, it is impossible to determine whether a project will receive revisions in the future, so a threshold of some kind is required to determine whether the project has indeed been abandoned for the purposes of the study.
This is where censoring is used.
Censoring involves using data from subjects of a study for which the time of interest is not observed.
There are multiple types of censoring in survival analysis, but this study uses random censoring or type III censoring.
Random censoring involves removing subjects (in this case, projects) from a study at varying times relative to when they began to be observed \cite{renganathan2016overview}. \keanu{find another citation to support this, http://www.stat.columbia.edu/~madigan/W2025/notes/survival.pdf mentions it, but refer to it as random type 1 right censoring}

The original authors set a time frame of 165 months (where a month is defined as 28 days), starting in 2005 and ending in January 2018.
This paper uses the same time frame and determines exact start and end dates.
Using January 1, 2018, as a strict end date and maintaining the study duration as 4620 days (165 months as defined), the start date is found to be May 9, 2005.

Projects are censored if there is reason to believe that they would receive revisions after the end of the time frame (i.e. the last \emph{observed} revision is unlikely to be the last revision of the project).
In this paper, any project with revisions on or after November 1, 2017, was censored. 
\keanu{NOTE: Our censoring method may change. This is because the paper \cite{ali2020cheating} conflicted with the video presentation \cite{ali2020video} with respect to the censoring method. After further inspection, it appears the method in the paper is superior, as we only censor data that truly had revisions after the time frame (though this probably will not affect many of the data points)}
The reason this is considered random censoring is that projects that are censored have varying start dates within the time frame of the study.
The distribution of the project timeline can be seen in Figure \ref{fig:figure-1}, which is a replication of Figure 1 in the original paper.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{figure1.jpg}
\caption{Graph of project timelines within the given time frame. The projects are ordered by duration and plotted from and to their respective start and end dates. Month 0 begins on May 9, 2005, and Month 165 concludes on January 1, 2018.}
\label{fig:figure-1}
\end{figure}

Using both the calculated duration associated with each project and the censoring status of each project, the survival analysis can be performed.
The analysis was carried out using an R notebook (this can be found in the project repository under Analysis\&Data/frequentist-analysis.rmd).

The K-M estimator is a non-parametric estimation technique that estimates the survival function, $S(t)$.
The survival function gives the probability that a given project will survive past a particular time $t$.
At $t = 0$, the K-M estimator is 1 and as $t$ approaches infinity, so does the K-M estimator.
More precisely, $S(t)$ is given by  $S(t) = p_0 \times p_1 \times p_2 \times \dots \times p_t$, where $p_i$ is the proportion of all projects that survived at the $i^{th}$ time step \cite{kaplan1958nonparametric}.\keanu{citation?}
The K-M estimator produces curves that approach the true survival function of the data.
The hazard function is another useful function in survival analysis. It describes the probability of the event of interest or hazard (project abandonment in this case) occurring up to a given time point.
The original paper uses the Cox Proportional-Hazards model which allows for fitting a regression model in order to better understand how the health of projects relate to their key attributes. 
This analysis results in the hazards ratio (HR), which is derived from the model for all covariates that are included in the formula. 
Briefly, a $HR > 1$ indicates an increased risk of abandonment; on the other hand, $HR < 1$, indicates a decreased risk of abandonment \cite{cox1972regression}. 
As such, the HR represents a relative risk of abandonment that compares one instance of a binary feature (e.g. yes or no) to the other instance.
\keanu{need citations for this section}
\manish{added citations}

\subsection{Revision Frequency Analysis} \label{revisionFreq}

The original paper mentions that ''The health of a project could be computed by the number and frequency of contributions...'' but never addresses this measurement.
This paper seeks to explore this method of assessment. 
The revision frequency, defined as the number of commits divided by the number of days in the project's observed lifetime, was dichotomized into two groups depending on whether the frequency was above one revision per day.
This study applies both the K-M estimator and the Cox Proportional-Hazards model to the data to stratify the effects of high revision frequency on the overall health of an open-source project.
\keanu{more to come...}

\subsection{Bayesian Survival Analysis}

The Bayesian approach to survival analysis is less common due to computational difficulties. However, it offers multiple advantages over the frequentist approach \cite{kelter2020bayesian}. 
We replicate the methods outlined in \cite{kelter2020bayesian} to apply Bayesian survival analysis to our study. 
The Bayesian analysis uses posterior distributions of model parameters to draw inferences about them. 
These posterior distributions are obtained via Markov-Chain-Monte-Carlo (MCMC) algorithms. 
The statistical modelling language used was Stan.

Our study applies the same methods as found in the section titled \emph{A detailed example} of \cite{kelter2020bayesian}. A parametric exponential model that assumes the survival times of a project $y = (y_1, y_2, \dots, y_n)$ are exponentially distributed with parameter $\lambda$ was created. 
The censoring indicators as $v = (v_1, v_2,\dots, v_n)$ where vi = 0 if yi is right censored (lost to follow-up) and vi = 1 if yi is a failure time (project abandonment), the survival function which is the probability of surviving past the time point yi is given by

\begin{equation}
S(y_i|\lambda) = P(T\geqyi|T\geq0) = 1 - [1 - \exp(-  \lambda y_i)] = \exp(- \lambda y_i)
\end{equation}

and the survival model is denoted as

\begin{equation}
y_i|v_i \sim f(y_i| \lambda)^{v_i} + S(y_i| \lambda)^{1-v_i} = [\lambda exp(-  \lambda y_i)]^{v_i} + [exp(-  \lambda y_i)]^{1-v_i}
\end{equation}
\begin{equation}
\lambda \sim p(\lambda)
\end{equation}
\begin{equation}
\lambda = exp(x_i^T \beta)
\end{equation}

This model was then used to visualize the posterior survival functions for the following five project attributes: major releases, hosting service of the project, use of multiple hosting services, team size, and revision frequency. 

% From proposal (slightly revised to make sense in this context):
% [This paper applies] the same methods as found in the section titled \emph{A detailed example} of \cite{kelter2020bayesian}. A parametric exponential model that assumes the survival times of a project $y = (y_1, y_2, \dots, y_n)$ are exponentially distributed with parameter $\lambda$ was created. This model was then used to visualize the posterior survival functions for the following five project attributes: major releases, hosting service of the project, use of multiple hosting services, team size, and revision frequency.
\section{Results} \label{results}

\subsection{Replication}


The replication study performed in this paper yielded extremely similar results to those shown in the original paper. 
Fig. 2 represents K-M curves along with their confidence interval and p-value which implies the survival probability of projects when grouped on the categorical value of each attribute. 
As seen in Fig. 2a, we found out that projects with at least one releases has higher chances of survival. 
The curve for projects having major releases plateaus around 65\% survival probability during the 120 month mark whereas the survival probability of projects with no major releases end up to be less than 20\% by the end of study period. 
Fig. 2b represents the significance of hosting the project on different hosting services, we found out that projects that are hosted on GitHub has a higher survival chances in the long run, as the curves suggest all three hosting services has similar trend for first 55 months which is within the average duration of projects hosted on these services.
We observed that having multiple repositories hosted on multiple services has significantly increased the chances of a project's survival.
As seen in Fig. 2c, the survival rate for such projects is 75\%, whereas around 20\% for projects with only one package repository system. 
The Curve in fig. 2d implies the significance of network of developers for survival of open source project, clearly the project having more than 20 different authors end up having around 60\% survival rate which are significantly different form projects with small team of developers with less than 20\% survival rate in the end of the study period.

The Table in figure 3 quantify estimates of these attribute's effect on the survival probability using the Cox Proportional-hazards model. 
Third column shows the hazards ratio which indicates the probability of abandonment with respect to the reference feature.
In the first raw we can the hazard ratio for projects not having a major release with respect to projects having major releases which is nearly 3, it implies that the projects without major releases are three times more likely to become inactive compared to projects with at least one major release. 
Similarly, projects having only one repository system are 3.3 times more likely to be abandoned.
The third row highlights that the projects with less developers count are 19.3 times more likely be inactive.
For the type of hosting used, we see that ratio implies that projects hosted on PyPi or Debian are less likely to be abandon compared to projects that are hosted on GitHub. This could be because Debian and PyPi-based projects have a higher survival rate during the first 55 months, the cox model considers this portion where most of the projects were participating heavily.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{major-releases.png}   
        \label{fig:major releases}
    \end{subfigure}
    \begin{subfigure}
        \centering 
        \includegraphics[width=0.45\textwidth]{host_type.png}
        \label{fig:host type}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure} 
        \centering 
        \includegraphics[width=0.45\textwidth]{multi_repo.png}
        \label{fig:multi repo}
    \end{subfigure}
    \begin{subfigure}   
        \centering 
        \includegraphics[width=0.45\textwidth]{img/author_count.png} 
        \label{fig:author count}
    \end{subfigure}
    \caption
    {\small KM curves of the python project data analyzed by major releases, host type, multiple repository hosting, and number of authors} 
    \label{fig:KM curves}
\end{figure*}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{cox-ratios.png}
\caption{The results of running the Cox Proportional hazards tool on the data. \keanu{we definitely want better descriptions for the images}}
\label{fig:Cox}
\end{figure}

\keanu{talk about significance and meaning of the KM and Cox results}

\keanu{As you can see in the Cox table summary, the numbers in each category are slightly different than the original study,
 we plan to look further into what has caused this difference in values}

\subsection{Revision Frequency Analysis}

The K-M curves for revision frequency are little different from the graphs we generated for other attributes, as seen in Fig 4. 
We observed curves drop for the projects with more than one revision per day in the starting of study period.
In Addition, the projects with less revision frequency out performed projects with higher revision frequencies during the mean duration period of the studied projects. 
As shown in the Fig. 3, results of our regression model also indicate that the projects with less revision frequency are less likely to be abandoned. 
This may be due to cox proportional model gives more consideration to the portion where most of the projects were active.

\keanu{It is worth noting the difference between this graph and the other graphs. The other ones indicate a clear ''winner'' whereas this has an interesting point where the functions cross. More investigation is required, but my initial interpretation is that, while many projects with hight commit frequencies are abondoned shortly after, the ones that do end up maturing seem to have a higher probability of surviving even longer than the other projects. Maybe consistency is the key here, not so much the frequency. There may not be any statistical significance to this result though, so we will look into it.}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.75]{commit_freq.png}
    \caption{KM curve for the data by revision frequency \keanu{need a better caption}}
    \label{fig:freq}
\end{figure}

\subsection{Bayesian Survival Analysis}

Shown in figure \ref{fig:Bayesian posterior survival functions} are the posterior survival functions for variations of the selected project attributes. The dotted lines represent the  2.5\% and 97.5\% quantiles, while the solid middle line represents the posterior mean of $\beta$ (the prior on the project attribute of interest). The remaining lines all represent valid posterior survival functions. 
Fig. 5a clearly indicates that the survival function of the projects with no major releases decreases much faster than projects with releases. 
Survival probability for projects with no releases was lower than 25\% after 150 months compared to 55\% for projects with releases.
Fig. 5b illustrates that projects hosted on Github have the highest survival chances compared to those hosted on Pypi and Debian. 
At 150 months, the predicted survival probabilities for Github, Pypi and Debian were 87, 65 and 25, respectively.
Fig. 5c shows that projects hosted on more than 1 version control system had significantly higher survival chances than projects hosted on a single hosting service. 
Above 65\% survival probability for multiple repositories compared to slightly over 25\% for single repository projects at the end of 150 months.
Fig. 5d demonstrates the importance of the number of contributing developers on the survival chances of open-source python projects. 
For projects with more than 20 authors, the predicted survival probability was around 55\%, while less than 25\% for projects with less than 20 authors at the 150-month mark.
Additionally, for all project attributes, the beta value for the 97.5\% quantile was found smaller than zero, which ensures that estimates are highly certain \cite{kelter2020bayesian}.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{major_releases_bayes.png}   
        \label{fig:major releases}
    \end{subfigure}
    \begin{subfigure}
        \centering 
        \includegraphics[width=0.45\textwidth]{host_type_bayes.png}
        \label{fig:host type}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure} 
        \centering 
        \includegraphics[width=0.45\textwidth]{multi_repo_bayes.png}
        \label{fig:multi repo}
    \end{subfigure}
    \begin{subfigure}   
        \centering 
        \includegraphics[width=0.45\textwidth]{author_count_bayes.png} 
        \label{fig:author count}
    \end{subfigure}
    \caption
    {\small Bayesian posterior survival functions of the python projects analyzed by major releases, host type, multiple repository hosting, and number of authors} 
    \label{fig:Bayesian posterior survival functions}
\end{figure*}

\section{Discussion} \label{discussion}

\subsection{Comparison of Analyses} \label{compare}

\keanu{We are not quite ready for a comparison yet}

\subsection{Limitations} \label{limit}

\subsubsection{Limitations of the Methods} \label{limit-methods}

The original paper \cite{ali2020cheating} and the MSR presentation given \cite{ali2020video} have contradicting methods of censoring.
The method discussed in the original paper was deemed superior and is used in this paper. \keanu{not used as of now, but will be}

Many details of the original paper's methods are left out for the sake of brevity.
This lead to assumptions being made when manipulating the data.
Hence, there are variations in the values retrieved compared to the original paper.
\keanu{NOTE: We are continuing to look into the possible causes of these inconsistencies. We will analyze our data-collection jupyter notebook and compare it with the end result of the R notebook to verify consistency between them before concluding it is an issue with the methods used}

% This following section is from the proposal and should be
% refined for the final report
Survival analysis methods such as the K-M estimator and the Cox Proportional-Hazards model have limitations of their own.
When applying the K-M estimator, it is common to use a log-rank test to test the significance between the two groups which are being compared.
The log-rank test only indicates whether or not the probability of survival is statistically significant between the two groups and is not able to provide any information about the size of the difference between the two groups \cite{stel2011kaplan}.
Additionally, the K-M estimator does not account for confounding factors \cite{stel2011kaplan}.
In more traditional uses of the K-M estimator, an example of a confounding factor could be the age of the study participants.
In the case of this study, there may be confounding factors such as the experience level of the developers or whether the developers received funding to work on the project.
Neither of these factors are represented in the data set.
The Cox Proportional-Hazards model is used with the assumption that, over the period of observation, the hazards within each group are proportional \cite{stel2011cox}.
If the assumption that the hazards within each group are proportional is not true, then the Cox Proportional-Hazards model will lead to incorrect estimates of the survival probability \cite{stel2011cox}. \keanu{we should assure the reader that it holds true in our case}

The Bayesian approach to survival analysis comes with its own limitations as well.
As pointed out by Renganathan, Bayesian survival analysis can be subjective as the analyst places their own bias into the model when selecting the prior distributions \cite{renganathan2016overview}.
In order to mitigate this bias, prior selection requires both epistemological and ontological reasoning. \keanu{We need to flesh out our reasoning more, probably written in the methods, but then perhaps referred to here}

\subsubsection{Limitations of the Data}

The data set in this study has been aggregated from multiple version control systems across the web over a large period of time.
As such, the data set is not fully reproducible, as pointed out by the original authors of the Software Heritage organization \cite{pietri2019software}.
Additionally, it cannot be ensured that the data contains a full history of the respective repositories.
The lack of certainty about the full history is because the repository admin can modify the history of revisions to suit their liking.
\keanu{add citation to perils of mining git}

There are inherent differences in the ways developers use the different hosting services \keanu{I have a strong intuition that this is the case, it looks like PyPi and debian don't have as much granularity compared to git and only show major releases, but I don't have anything to back this up, so maybe we can find something to cite for that}.
This could skew the results because it may be the case that services such as PyPi and Debian are primarily used to host major releases of a product.
This may hide information about the number of developers and the revision frequency.
Additionally, the potential confounding factors mentioned in section \ref{limit-methods} are not represented in this data set.

It may also be worth noting that this data is only for Python projects and that it is possible that different behaviours are associated with development in different languages.
Python is a relatively easy language to use, and can often be used for small tasks that are not maintained.
However, because this data set comprises popular projects, it is unlikely to contain projects that were used for a small task and discarded.

There was no clear method for determining whether a project was hosted in multiple repositories.
The only unique identifier for each project was a url, from which the project name was extracted using regular expressions.
This may have resulted in the extraction of inconsistent project names across host types.
The assumption is that each project that was hosted on multiple repositories would be given the exact same name, and that projects of the same name hosted on different services were indeed the same project (which may not always be the case).
Additionally, it is possible for users on Github to give their projects the exact same name as pre-existing projects created by other users.
This issue was accounted for in this paper's analysis, but it is unclear whether the original authors accounted for this.

The data contains a large portion of censored data.
This means the abandonment of most of the projects was not observed.
As data points are censored (denoted by the vertical tick marks in the KM curves), there is a smaller and smaller group of data points to study.
This means that the results towards the 165 month mark may be less representative.
This is to be expected with any SE study, as recent years have lead to an exponential increase in the number of OSS projects \keanu{find citation?}.


The data set contained many revisions (over 4 million) that were not associated with project URLs.
It is unclear how this happened.

% Not sure we need this
%\subsubsection{Replication Challenges}

\subsection{Future Work} \label{future}

\keanu{Just a couple of ideas so far}

Increase the time frame of the study (the original paper could not do this because the paper was written in 2018).

Perform separate studies on each hosting service to remove variability in the way the services are utilized.

\section{Conclusion} \label{conclusion}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{../refs/refs.bib}

%%
%% If your work has an appendix, this is the place to put it.
\appendix
\section*{APPENDICES}
\section{Artifacts} \label{artifacts}

Project Repository: \url{https://github.com/DerekRobin/CSC578B-Project}\\
Data Set: \url{https://annex.softwareheritage.org/public/dataset/graph/latest/popular-3k-python/sql/}

\section{Work Distribution}

\end{document}
\endinput
