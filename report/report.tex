\documentclass[acmconf]{acmart}
% % Packages
%\usepackage{cite}
\usepackage{balance} % For balanced columns on the last page
\usepackage{listings}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
%\usepackage{parskip}
%\setlength{\parskip}{1em}
%\usepackage[bottom]{footmisc}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{totpages}
%\usepackage{subfigure}
\usepackage{subcaption}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{10.1145/1122445.1122456}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%   June 03--05, 2018, Woodstock, NY}
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

% Specify path to images
\graphicspath{ {./img/} }

% import custom commands found in commands.tex
\input{commands}
% set up commands to format RQ nicely
\newlist{questions}{enumerate}{2}
\setlist[questions,1]{label=\textbf{RQ\arabic*.},ref=RQ\arabic*}
\setlist[questions,2]{label=(\alph*),ref=\thequestionsi(\alph*)}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Survival Analysis of Open Source Projects]{Two Differing Approaches to Survival Analysis of Open Source Python Projects}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author{Derek Robinson}
\email{drobinson@uvic.ca}
\author{Keanelek Enns}
\email{keanelekenns@uvic.ca}
\author{Neha Koulecar}
\email{nehakoulecar@uvic.ca}
\author{Manish Sihag}
\email{manishsihag@uvic.ca}
\affiliation{%
  \institution{\\University of Victoria}
  \department{Computer Science}
  \streetaddress{PO Box 1700 STN CSC}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8W 2Y2}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{D. Robinson, K. Enns, N. Koulecar, M. Sihag}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\keanu{Abstract Pending}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011134.10003559</concept_id>
<concept_desc>Software and its engineering~Open source model</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003227.10003351</concept_id>
<concept_desc>Information systems~Data mining</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Open source model}
\ccsdesc[300]{Information systems~Data mining}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{data science, survival analysis, open source, python, Kaplan Meier, Cox proportional hazards model, Bayesian analysis}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction} \label{intro}

The developers of Open Source Software (OSS) projects are often part of decentralized and geographically distributed teams of volunteers.
As these developers volunteer their free time to build such OSS projects, they likely want to be confident that the projects they work on will not become inactive.
Suppose OSS developers are aware of key attributes that are associated with long-lasting projects. In that case, they can make informed assessments of a given project before devoting their time to it, or they can strive to make their own projects exhibit those attributes.
Understanding which attributes of an OSS project lead to its longevity motivated Ali \emph{et al.} to apply survival analysis techniques commonly found in biostatistics to study the probability of survival for popular OSS Python projects \cite{ali2020cheating}.
Ali \emph{et al.} (referred to as the original authors from here on) specifically studied the effect of the following attributes on the survival of OSS Python projects: publishing major releases, the use of multiple hosting services, the type of hosting service, and the size of the volunteer team.

Survival analysis is a set of methods used to determine how long an entity will live (or the time to a given event of interest, such as death) and is often used in the medical field.
For example, survival analysis can determine the probability of a patient surviving past a certain time when given a treatment.
However, death is not as well defined for a software project as it is for a living organism.
A project may not receive revisions for an extended period only to be returned to at a later date, or perhaps a project no longer receives revisions at all, but the community that uses it continues to be active.
Samoladas \emph{et al.} considered a project inactive if it received less than two revisions a month; two months of inactivity led to it being considered abandoned or dead \cite{samoladas2010survival}.
Evangelopoulos \emph{et al.} and the original authors considered a project dead once there were no revisions at all \cite{evangelopoulos, ali2020cheating}.
The latter definition of project abandonment or death is used in this study to measure the duration of a project or its survival.

The original authors use a frequentist approach to survival analysis utilizing such methods as the Kaplan-Meier (K-M) survival estimator and the Cox Proportional-Hazards model \cite{kaplan1958nonparametric, cox1972regression}.
Though frequentist approaches are considered to be unbiased, minimal in variance, efficient, and generally sufficient, some consider them to lack robustness \cite{renganathan2016overview}. Another approach to survival analysis, Bayesian analysis, is considered to generate more robust models that perform well under the presence of new data being introduced and are generally easier to interpret results from \cite{renganathan2016overview}.

The authors of this paper resonate with the motivation of the original authors. 
This paper serves as a replication of their paper \cite{ali2020cheating} (referred to as the original paper from here on) and seeks to assess the validity of their analysis.
This replication also provides artifacts so that others may see how the study was conducted and reproduce it with ease.
For the sake of the authors' curiosity, an additional attribute of the data was analyzed: the revision frequency of the project.
In addition to the replication, this paper analyzes the same data set using a Bayesian approach to survival analysis as outlined in \cite{kelter2020bayesian} and seeks to compare the results of the frequentist and Bayesian approaches in the same domain.
Thus, the research questions this paper answers are as follows:

\begin{questions}
    \item How do major releases, the use of multiple hosting services, the type of hosting service, and the size of the volunteer team affect the probability of survival of an OSS Python project?
    \item How does the revision frequency of an OSS Python project affect the probability of its survival?
    \item How do the findings of frequentist survival analysis differ from Bayesian survival analysis?
\end{questions}

Section \ref{related} outlines other research which has utilized survival analysis to study OSS and other work which has studied attributes similar to those studied in this paper.
Section \ref{data} describes the source of the data set, the data set itself, and the required preparation in order to perform survival analysis.
Section \ref{methods} covers the methods used for the replication, Bayesian analysis, and the additional attribute analysis.
Section \ref{results} shows the results for each analysis.
Section \ref{discussion} discusses the results, implications, and limitations, and gives suggestions for future work.
The final section concludes by summarizing the purpose and findings of this paper.

\section{Related Work} \label{related}

Several other researchers have employed survival analysis to study the health of OSS projects. 
For example, Samoladas \emph{et al.} studied the affect of application domain and developer count on OSS project health \cite{samoladas2010survival}. 
They found that applications within the domain of \emph{games and entertainment} and \emph{security} had the lowest probability of survival. 
Additionally, they found that for each new developer introduced to a project, the projects survivability increased by 15.8\%. 
On the topic of developers, several studies have used survival analysis to study developer disengagement from OSS projects \cite{miller2019people,lin2017developer,ortega2009survival}. 
Miller \emph{et al.} made use of a survey and survival analyses, to determine the causes behind why a developer might stop contributing to an OSS project \cite{miller2019people}. 
Their analysis revealed that developers have a higher probability of project disengagement when going through job transitions and when working longer hours. 
Lin \emph{et al.} determined that developers who balance maintaining files they have created with maintaining files others have created have a higher survival probability than developers who only maintain their own files or only maintain others files \cite{lin2017developer}. 
Additionally, Lin \emph{et al.} found that developers who maintained files and developers who mainly wrote code had a higher survival probability than those who solely created files and those whose main focus was writing documentation. 
Ortega and Izquierdo-Cortazar analyzed the survival of FLOSS committers and Wikipedia editors and found that FLOSS committers have higher mean survival times than Wikipedia editors \cite{ortega2009survival}. 
Survival analysis can also be applied to the software it self, this has been demonstrated by Aman \emph{et al.} and Caivano \emph{et al.} \cite{aman2017survival, caivano2021exploratory}. 
In \cite{aman2017survival}, survival analysis was used to analyze time to bug-fix for files modified by developers of different experience levels. 
This analysis determined that files which most recently modified by less experienced developers had an increased probability of needing a bug fix within a shorter time frame \cite{aman2017survival}. 
Caivano \emph{et al.} explored the affect of dead code within OSS projects using survival analysis \cite{caivano2021exploratory}. 
They found that dead methods are present in Java code, persist for a long time (in terms of commits) before being buried or revived, are rarely revived, and that most dead methods have been dead since their inception. 

Other studies have examined the health of OSS projects using methods other than survival analysis. 
Xia \emph{et al.} predicted a number of health indicators of OSS projects, such as, the number of developers and the number of revisions. 
These predictions were made using regression trees that were optimized using differential evolution, leading to a 10\% increase in prediction accuracy over the base line \cite{xia2020predicting}. 
Norick \emph{et al.} analyzed OSS projects using code quality measures and observed no significant evidence that the number of committing developers affects software quality \cite{norick2010effects}. 

\section{Data} \label{data}

Performing survival analysis of OSS projects requires a data set that records the repositories for projects on common hosting sites, including a history of all commits (revisions from here on out) and major releases (revisions of note, often with a specific name and release date) \cite{ali2020cheating}. 
The \emph{popular-3k-python} subset of the Software Heritage graph \cite{pietri2019software} contains the necessary information and was used in the original paper and also in this paper.
This data set contains information on 3052 popular Python projects hosted on GitHub/GitLab, Debian, and PyPI, and records revisions between 1980 and 2019 at the time of writing (the Software Heritage graph is subject to updates, which makes it a non-reproducible data set).
Following the tutorial provided by the Software Heritage organization \cite{SQLdataset}, a PostgreSQL database was hosted on the authors' local machines to facilitate data collection.

Though the \emph{popular-3k-python} data set contains all the necessary information to perform survival analysis, it first must be manipulated into a more suitable format before the analysis can be carried out. 
In this case, the collected data was manipulated such that the final data set contained the duration of the project, the censorship value, and the attributes of interest. 
Descriptions of each column present in the data set can be found in Table \ref{tab:data}. 
Data collection and manipulation were performed in Jupyter Notebooks, which are available in this paper's repository, a link to which can be found in appendix \ref{artifacts}. 

\begin{table}[!ht]
    \caption{Data Set Column Descriptions}
    \label{tab:data}
    \begin{tabular}{ll}
        \toprule
        Column Name & Description \\
        \midrule
        Host Type            & Which hosting service the project's repository resides on \\
        Major Releases       & Whether or not the project publishes major releases \\
        Censored             & True if the project's death is not observed (for more information see section \ref{death_censoring}) \\
        Duration\_months     & The duration of the project in months \\
        High\_rev\_frequency & Whether or not the project has high revision frequency (greater than one revision per day) \\
        Multi\_repo          & Whether or not the project is hosted on multiple hosting services\\
        High\_author\_count  & Whether or not the project has a high author count (greater than twenty unique authors) \\
        \bottomrule
    \end{tabular}
\end{table}

For their study, the original authors set a time frame of 165 months (where a month is defined as 28 days), starting in 2005 and ending in January 2018.
This paper uses the same time frame and determines exact start and end dates.
Using January 1, 2018, as a strict end date and maintaining the study duration as 4620 days (165 months as defined), the start date is found to be May 9, 2005.
After following the same procedures described in the original paper, a list of 2066 projects and their associated information was obtained.

%For example, most of the analyzed features or attributes of the projects in the original paper are not present in the raw data set and need to be calculated. 
%The work done in this study has been documented in a repository for the benefit of the reader.
%References to the repository can be found in appendix \ref{artifacts}, and the jupyter notebook used to perform the data manipulation can be found in Analysis\&Data/data-collection.ipynb.

%Queries were created in order to join the necessary tables and extract the relevant values.
%The queries accomplish the following objectives: they group records by the \emph{url} field of the origin table (used to identify projects), filter records to be within the desired time frame, count distinct authors that have participated in a given project, record the host service (depicted by the \emph{type} field of the origin table), record the earliest and latest revisions associated with each project url, count the number of revisions made during each project's duration, and identify whether major releases were published.

%The notebook uses popular python libraries such as psycopg2, pandas, numpy, and matplotlib to connect to the database, extract the results of the previously mentioned queries, manipulate and filter the data, calculate new fields, and plot results.

\section{Methods} \label{methods}

\subsection{Replication} \label{replmethods}
\subsubsection{Death and Censoring} \label{death_censoring}

Two critical concepts in survival analysis are events of interest and censoring.
As previously discussed, the event of interest for this study is project abandonment or death.
As defined in the introduction, a project is considered dead when it no longer receives any revisions.
With this definition, it is impossible to know whether any project is truly dead, because, unlike a living organism, any project may receive revisions at any point in the future, making it "not dead" by the working definition.
However, there are multiple practical ways of determining whether a project is dead, all of which rely on the scope of the studied data set.
For this study, the death of a project is determined by first defining two special revisions for each project: \emph{last revision} and \emph{last observed revision}. 
\emph{Last revision} is defined as the last recorded revision of a project within the scope of the data set (i.e. 1980 - 2019). 
\emph{Last observed revision} is defined as the last recorded revision of a project within the studied time frame (i.e. 2005 - 2018). 
If, and only if, the \emph{last revision} is also the \emph{last observed revision}, the project death is said to be observed. 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.4]{figure1.jpg}
\caption{
    Graph of project durations within the studied time frame.
    The projects are ordered by duration and plotted from and to their respective start and end dates.
    Month 0 begins on May 9, 2005, and Month 165 concludes on January 1, 2018.
    The black portion of the horizontal lines indicates the active period of a given project
    }
\label{fig:figure-1}
\end{figure}

What happens if a project's death is not observed?
This is where censoring is used.
Suppose a project has its first revision one month before the end of the studied time frame, and continues to be revised daily past the studied time frame.
It would be incorrect to indicate that this project survived for only one month, but it is also impossible to include the project's future activity in the study.
Rather than discarding such projects, censoring causes them to be considered by the study for the observed duration, but without considering them as dead projects (i.e. they are removed from the calculations after they stop being observed).
There are multiple types of censoring in survival analysis, but this study uses random censoring or type III censoring, which involves removing subjects from a study at varying times relative to when they began to be observed (as is the case here) \cite{renganathan2016overview}. If a project's death is not observed, it is considered censored.

The distribution of the project durations over the studied time frame can be seen in Fig. \ref{fig:figure-1}, which is a more detailed rendition of Figure 1 in the original paper.
Note that when the black lines extend to the end of the time frame, this likely indicates the project was censored.
About 62\% of the studied projects were censored.

\subsubsection{Survival Analysis}
Using both the calculated duration associated with each project and the censoring status of each project, the survival analysis can be performed.
The analysis was carried out using an R notebook (this can be found in the project repository under Analysis\&Data/frequentist-analysis.rmd).

The K-M estimator is a non-parametric estimation technique that estimates the survival function, $S(t)$.
The survival function gives the probability that a given project will survive past a particular time $t$.
At $t = 0$, the K-M estimator is 1 and as $t$ approaches infinity, so does the K-M estimator.
More precisely, $S(t)$ is given by  $S(t) = p_0 \times p_1 \times p_2 \times \dots \times p_t$, where $p_i$ is the proportion of all projects that survived at the $i^{th}$ time step \cite{kaplan1958nonparametric}.
The K-M estimator produces curves that approach the true survival function of the data.

The hazard function is another useful function in survival analysis. It describes the probability of the event of interest or hazard (project abandonment in this case) occurring up to a given time point.
The original paper uses the Cox Proportional-Hazards model which allows for fitting a regression model in order to better understand how the health of projects relate to their key attributes. 
This analysis results in the hazards ratio (HR), which is derived from the model for all covariates that are included in the formula. 
Briefly, a $HR > 1$ indicates an increased risk of abandonment; on the other hand, $HR < 1$, indicates a decreased risk of abandonment \cite{cox1972regression}. 
As such, the HR represents a relative risk of abandonment that compares one instance of a binary feature (e.g. yes or no) to the other instance.

\subsection{Bayesian Survival Analysis}

The Bayesian approach to survival analysis is less common due to computational difficulties. 
However, it offers multiple advantages over the frequentist approach \cite{kelter2020bayesian}. 
We replicate the methods outlined in \cite{kelter2020bayesian} to apply Bayesian survival analysis to our study. 
The Bayesian analysis uses posterior distributions of model parameters to draw inferences about them. 
These posterior distributions are obtained via Markov-Chain-Monte-Carlo (MCMC) algorithms. 
The statistical modelling language used was Stan.

Our study applies the same methods as found in the section titled \emph{A detailed example} of \cite{kelter2020bayesian}. 
A parametric exponential model that assumes the survival times of a project $y = (y_1, y_2, \dots, y_n)$ are exponentially distributed with parameter $\lambda$ was created as shown in Equation \ref{eq:surv_times}.

\begin{equation} \label{eq:surv_times}
    f(y_i|\lambda) = \lambda\exp(- \lambda y_i) \mbox{ for } i=1,\dots,n
\end{equation}

The censoring indicators are denoted as $v = (v_1, v_2,\dots, v_n)$ where $v_i = 0$ if $y_i$ is right censored (project death is not observed) and $v_i = 1$ if $y_i$ is a failure time (project death is observed) and the survival function which is the probability of surviving past the time point $y_i$ is given by Equation \ref{eq:surv_func}.

\begin{equation} \label{eq:surv_func}
    S(y_i|\lambda) = P(T \geq yi|T \geq 0) = 1 - [1 - \exp(-  \lambda y_i)] = \exp(- \lambda y_i)
\end{equation}

Finally, the survival model is given by Equation \ref{eq:model}.

\begin{equation} \label{eq:model}
    y_i|v_i \sim f(y_i| \lambda)^{v_i} + S(y_i| \lambda)^{1-v_i} = [\lambda exp(-  \lambda y_i)]^{v_i} + [exp(-  \lambda y_i)]^{1-v_i}
\end{equation}
\begin{equation}
    \lambda \sim p(\lambda)
\end{equation}
\begin{equation}
    \lambda = exp(x_i^T \beta)
\end{equation}
\begin{equation}
    \beta = normal(0, 10)
\end{equation}

This model was then used to visualize the posterior survival functions for the following five project attributes: major releases, hosting service of the project, use of multiple hosting services, team size, and revision frequency. 

\subsection{Revision Frequency Analysis} \label{revisionFreq}

The original paper mentions that ''The health of a project could be computed by the number and frequency of contributions...'' but never addresses this measurement.
This paper seeks to explore the frequency of contributions as a method of assessment (simply analyzing the number of contributions would not yield useful results given the varying nature of the project durations in the studied time frame). 
The revision frequency, defined as the number of commits divided by the number of days in the project's observed lifetime, was dichotomized into two groups depending on whether the frequency was above one revision per day.
Although the median revision frequency was approximately 0.68 revisions per day, the threshold value of one was chosen because it is easier to remember when keeping these attributes in mind and, similar to the other dichotomizing attributes, it provides a threshold that fewer projects attain to, which sets them apart.
This study applies both the K-M estimator and the Cox Proportional-Hazards model to the data to stratify the effects of high revision frequency on the overall health of an open-source project.

\section{Results} \label{results}

\subsection{Replication}

The replication study performed in this paper yielded extremely similar results to those shown in the original paper. 
Fig. \ref{fig:KM curves} depicts K-M curves along with their confidence intervals and p-values. 
The p-values imply that the difference in survival probability for projects within each group is statistically significant.
As seen in Fig. \ref{fig:major_releases}, this study found out that projects with at least one major release have higher chances of survival. 
The curve for projects with at least one major releases plateaus around 65\% survival probability around the 120-month mark whereas the survival probability of projects with no major releases ends up to be less than 20\% by the end of the study period. 
Fig. \ref{fig:host_type} represents the significance of hosting the project on different hosting services, it was observed that projects that are hosted on GitHub have higher survival chances in the long run, as the curves suggest all three hosting services have a similar trend for the first 55 months which is within the average duration of projects hosted on these services.
In addition, having multiple repositories hosted on multiple services has significantly increased the chances of a project's survival.
As seen in Fig. \ref{fig:multi_repo}, the survival rate for such projects is 75\%, whereas around 20\% for projects with only one package repository system. 
The Curve in fig. \ref{fig:author_count} implies the significance of network of developers for the survival of open source project, the project having more than 20 different authors end up having around 60\% survival rate which is significantly different from projects with a small team of developers with less than 20\% survival rate at the end of the study period.

Figure \ref{fig:Cox} quantify estimates of these attributes' effect on the survival probability using the Cox Proportional-hazards model. 
The third column shows the hazards ratio which indicates the probability of abandonment with respect to the reference feature.
In the first row, the hazard ratio for projects not having a major release with respect to projects having major releases is nearly 3, it implies that the projects without major releases are three times more likely to become inactive compared to projects with at least one major release. 
Similarly, projects having only one repository system are 3.3 times more likely to be abandoned.
The third row highlights that the projects with fewer developers count are 19.3 times more likely to be inactive.
For the type of hosting used, the ratio implies that projects hosted on PyPi or Debian are less likely to be abandoned compared to projects that are hosted on GitHub. 
This appears to contradict the results of the K-M curve and will be further discussed in Section \ref{implications}.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{img/KM-major_release.jpg}
        \caption{\small K-M curves for projects which publish major releases and those which do not} 
        \label{fig:major_releases}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering 
        \includegraphics[width=\textwidth]{img/KM-host_type.jpg}
        \caption{\small K-M curves for projects with different hosting services} 
        \label{fig:host_type}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering 
        \includegraphics[width=\textwidth]{img/KM-multi_repo.jpg}
        \caption{\small K-M curves for projects with repositories on multiple hosting services} 
        \label{fig:multi_repo}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth} 
        \centering 
        \includegraphics[width=\textwidth]{img/KM-author_count.jpg}
        \caption{\small K-M curves for projects with high and low author count} 
        \label{fig:author_count}
    \end{subfigure}
    \caption{\small K-M curves for each project attribute} 
    \label{fig:KM curves}
\end{figure}

\begin{figure*}[!ht]
\centering
\includegraphics[scale=0.5]{img/CoxHR-table.jpg}
\caption{The Cox Proportional Hazards model. From left to right: project attribute, attribute value with counts, hazard ratio, box plot of hazard ratio, and p-value of log rank test.}
\label{fig:Cox}
\end{figure*}

\subsection{Bayesian Survival Analysis}

Shown in Fig. \ref{fig:Bayesian posterior survival functions} are the posterior survival functions for variations of the selected project attributes. 
The dotted lines represent the  2.5\% and 97.5\% quantiles, while the solid middle line represents the posterior mean of $\beta$ (the prior on the project attribute of interest). 
The remaining lines all represent all valid posterior survival functions. 
Fig. \ref{fig:bayes_major_releases} clearly indicates that the survival function of the projects with no major releases decreases much faster than projects with releases. 
Survival probability for projects with no major releases was lower than 25\% after 150 months compared to 55\% for projects with major releases.
Fig. \ref{fig:bayes_host_type} illustrates that projects hosted on Github have the highest survival chances compared to those hosted on PyPi and Debian. 
At 150 months, the predicted survival probabilities for Github, PyPi and Debian were 87\%, 65\% and 25\%, respectively.
Fig. \ref{fig:bayes_multi_repo} shows that projects with repositories on more than 1 hosting service had significantly higher survival chances than projects with repositories on a single hosting service. 
Above 65\% survival probability for multiple repositories compared to slightly over 25\% for single repository projects at the end of 150 months.
Fig. \ref{fig:bayes_author_count} demonstrates the importance of the number of contributing developers on the survival chances of open-source python projects. 
For projects with more than 20 authors, the predicted survival probability was around 55\%, while less than 25\% for projects with less than 20 authors at the 150-month mark.
Additionally, for all project attributes, the beta value for the 97.5\% quantile was smaller than zero, which ensures that the estimates for $\lambda$ have low uncertainty \cite{kelter2020bayesian}.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{major_releases_bayes.png}   
        \caption{\small Posterior survival functions for projects which publish major releases and those which do not}
        \label{fig:bayes_major_releases}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering 
        \includegraphics[width=\textwidth]{host_type_bayes.png}
        \caption{\small Posterior survival functions for projects with different hosting services}
        \label{fig:bayes_host_type}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering 
        \includegraphics[width=\textwidth]{multi_repo_bayes.png}
        \caption{\small Posterior survival functions for projects with repositories on multiple hosting services}
        \label{fig:bayes_multi_repo}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering 
        \includegraphics[width=\textwidth]{author_count_bayes.png} 
        \caption{\small Posterior survival functions for projects with high and low author count}
        \label{fig:bayes_author_count}
    \end{subfigure}
    \caption{\small Bayesian posterior survival functions for each project attribute} 
    \label{fig:Bayesian posterior survival functions}
\end{figure}

\subsection{Revision Frequency Analysis} \label{rev-freq-results}

The K-M curves for revision frequency are little different from the graphs we generated for other attributes, as seen in Fig 4. 
We observed curves drop for the projects with more than one revision per day in the starting of study period.
In Addition, the projects with less revision frequency out performed projects with higher revision frequencies during the mean duration period of the studied projects.
Fig. 4 also shows Bayesian survival functions for projects with more than one revision per day and those with revision per day less than or equal to one. Although a significant difference was not observed in predicted survival probabilities, projects with more than one revision per day had a slightly higher chance of survival by the study period's end.
As shown in the Fig. 3, results of our regression model also indicate that the projects with less revision frequency are less likely to be abandoned. 

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{img/KM-rev_freq.jpg}   
        \label{fig:KM_curve_for_revision_frequency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering 
        \includegraphics[width=\textwidth]{img/commit_freq_bayes.png}
        \label{fig:Bayesian_curve_for_revision_frequency}
    \end{subfigure}
    \caption{\small K-M (left) and Bayesian posterior survival function (right) curves for revision frequency} 
    \label{fig:Revision frequency}
\end{figure}

\section{Discussion} \label{discussion}
As in other works, the author count was found to be the most significant indicator of a project's health.
Interestingly, Norick et al. observed no significant evidence that software quality is affected by the number of developers.
This suggests that a higher number of developers has some other affect on the longevity of a project that is unrelated to the quality of the software.

The cox proportional hazards assumption does not hold.
This means the hazard ratios are not applicable to the entire studied time frame.
It is assumed that the calculations in the cox model give more weight to the portion of the study containing a larger number of subjects (i.e. the left most portion of the KM curves).

The bayesian model forces the data to conform to the chosen distribution and cannot reflect more complex detail.

Talk about the drop in the revision frequency KM curve (short lived projects with rapid development).
Talk about crossing over point.
It appears that having a higher revision frequency benefits a project in the long run.
Maybe this indicates that consistency is more important.
Discuss difference between bayesian and KM for this attribute.
However, the bayesian analysis agrees with the KM curve in that projects with a higher revision frequency outlast the other projects despite having the sudden drop.


Mention 80 month critical point hypothesis, this could be future work

\subsection{Implications} \label{implications}

The results of this study, and the studies it is based on, have practical applications.
They give developers of open source software insights into which projects are likely to receive long term attention from other developers.
They help project coordinators understand the attributes that their projects should possess if they want a long-lasting project.
They give companies and organizations looking to invest in or utilize or rely on open source projects confidence that such projects will remain active.

Though our results were qualitatively similar to those of the original authors, it is suspected that a variation in approaches for data retrieval and the analysis tools used caused quantitative discrepancies (refer to 19.3 HR for author count vs 5.95).
We therefore call for researchers to improve the reproducibility of their studies by providing artifacts.


The application of Bayesian survival analysis presented in this paper is meant to surve as a preliminary investigation of the method in the domain of software.
It is suggested that researchers looking to use this method in the future take a deeper look into priors, likelihoods, and covariates.

\subsection{Limitations} \label{limit}

\subsubsection{Limitations of the Methods} \label{limit-methods}

The original paper \cite{ali2020cheating} and the MSR presentation given \cite{ali2020video} have contradicting methods of censoring.
The method discussed in the original paper was deemed superior and is used in this paper.

The original authors left out many details regarding the methods they used for data extraction, manipulation, and analysis.
This led to assumptions being made when replicating their study.
Hence, there are discrepancies in the values obtained compared to the original paper.

Being a replication, this study was limited to choosing the methods used by the original paper.
These methods are also unanimous in the area of survival analysis, and there are few alternatives to them.
This study attempts to address these limitations by providing an alternative analysis method, namely Bayesian analysis.

When applying the K-M estimator, it is common to use a log-rank test to test the significance between the two groups which are being compared.
The log-rank test only indicates whether or not the probability of survival is statistically significant between the two groups and is not able to provide any information about the size of the difference between the two groups \cite{stel2011kaplan}.
Additionally, the K-M estimator does not account for confounding factors \cite{stel2011kaplan}.
In more traditional uses of the K-M estimator, an example of a confounding factor could be the age of the study participants.
In the case of this study, there may be confounding factors such as the experience level of the developers or whether the developers received funding to work on the project.
Neither of these factors are represented in the data set.

The Cox Proportional-Hazards model is used with the assumption that, over the period of observation, the hazards within each group are proportional \cite{stel2011cox}.
If the assumption that the hazards within each group are proportional is not true, then the Cox Proportional-Hazards model will lead to incorrect estimates of the hazard ratio between two groups \cite{stel2011cox}.
Looking at the K-M curves for this study, it can be concluded that the proportional hazards assumption does not hold as the survival functions diverge over time and cross over each other rather than running in parallel \cite{persson2007ACO}.
This explains the discrepancies between the results of the cox regression model and the K-M curves.
Future studies should perform tests to determine whether the assumptions for their models hold and should seek methods for mitigating such errors through identifying time-dependant covariates.

The Bayesian approach to survival analysis comes with its own limitations as well.
As pointed out by Renganathan, Bayesian survival analysis can be subjective as the analyst places their own bias into the model when selecting the prior distributions \cite{renganathan2016overview}.
In order to mitigate this bias, prior selection requires both epistemological and ontological reasoning.
Prior distributions were chosen based on survival analysis done in other domains \cite{kelter2020bayesian, rethinking}, but more investigation should be done as to whether these priors are appropriate for this domain and whether the models from this study accurately predict durations of different data sets.

\subsubsection{Limitations of the Data}

The data set in this study has been aggregated from multiple version control systems across the web over a large period of time.
As such, the data set is not fully reproducible, as pointed out by the original authors of the Software Heritage organization \cite{pietri2019software}.
Additionally, it cannot be ensured that the data contains a full history of the respective repositories.
The lack of certainty about the full history is because the repository admin can modify the history of revisions to suit their liking \cite{perils2009}.

There are inherent differences in the ways developers use the different hosting services.
Traditionally, services such as PyPi and Debian are used to host major releases of a product.
This may hide information about the number of developers and the revision frequency.
Additionally, the potential confounding factors mentioned in section \ref{limit-methods} are not represented in this data set.

It may also be worth noting that this data is only for Python projects and it is possible that different behaviours are associated with development in different languages.
Python is a relatively easy language to use, and can often be used for small tasks that are not maintained.
The results of the revision frequency analysis in section \ref{rev-freq-results} seem to indicate a large number of short lived projects.

The data set contains a large portion of censored data.
This means the abandonment of most of the projects was not observed.
As data points are censored (denoted by the vertical tick marks in the KM curves), there is a smaller and smaller group of data points to study.
This means that the results towards the 165 month mark may be less representative.

The data set contained many revisions (over 4 million) that were not associated with project URLs.
The cause of this is unclear.

% Not sure we need this
%\subsubsection{Replication Challenges}
\subsection{Future Work} \label{future}

It can be said with high confidence that the this is the first application of Bayesian survival analysis in the context of OSS. 
As such, prior distributions were uninformative and priors and likelihoods alike were chosen based on the application of Bayesian survival analysis done in other domains. 
More investigation should be conducted to determine if the priors and likelihoods applied in this paper are suitable for OSS. 
Further more, future work should investigate the predictive power of the Bayesian survival model presented here to determine its performance on other data sets. 
Similarly, an evaluation of the assumptions present in the Cox Proportional Hazards model should performed to determine if these assumptions are reasonable for the domain of OSS.

In regards to study design, future work should increase the time frame of the study to determine the how the attributes analyzed in this paper play out over a longer time period. 
Similarly, as only OSS written in Python was studied here, future work should evaluate the survival of OSS written in other popular languages such as Java. 

As noted in the discussion, a higher number of developers has some effect on the longevity of a project that is not related to the quality of the software being developed. 
Future works should investigate the reason behind this increased longevity and attempt to determine a reason for it. 
In line with Samoladas \emph{et al.}, distinguishing between core developers and sporadic ones could lead to an increased understanding of the aforementioned phenomena. 

\keanu{Remove short lived projects, e.g. a day long}

\keanu{this study considered the first observed revision to be the first revision of a project (similar definitions when compared to those in section \ref{death_censoring}), but this may not always be the case. Future studies should acknowledge the true beginning revision for projects in order to obtain a more accurate duration value as in \cite{samoladas2010survival}.}

\section{Conclusion} \label{conclusion}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{../refs/refs.bib}

%%
%% If your work has an appendix, this is the place to put it.
\appendix
\section*{APPENDICES}
\section{Artifacts} \label{artifacts}

Project Repository: \url{https://github.com/DerekRobin/CSC578B-Project}\\
Data Set: \url{https://annex.softwareheritage.org/public/dataset/graph/latest/popular-3k-python/sql/}

\section{Work Distribution}

\end{document}
\endinput